# -*- coding: utf-8 -*-
"""AdvanceML_EX3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FK_tVkp0WtwMDOvdNwOZ__GXPkOtU852
"""

"""### **Code from models.py**"""

import torch
import torch.nn as nn
from torchvision.models import resnet18


class Encoder(nn.Module):
    def __init__(self, D=128, device='cuda'):
        super(Encoder, self).__init__()
        self.resnet = resnet18(pretrained=False).to(device)
        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=1)
        self.resnet.maxpool = nn.Identity()
        self.resnet.fc = nn.Linear(512, 512)
        self.fc = nn.Sequential(nn.BatchNorm1d(512), nn.ReLU(inplace=True), nn.Linear(512, D))

    def forward(self, x):
        x = self.resnet(x)
        x = self.fc(x)
        return x

    def encode(self, x):
        return self.forward(x)


class Projector(nn.Module):
    def __init__(self, D, proj_dim=512):
        super(Projector, self).__init__()
        self.model = nn.Sequential(nn.Linear(D, proj_dim),
                                   nn.BatchNorm1d(proj_dim),
                                   nn.ReLU(inplace=True),
                                   nn.Linear(proj_dim, proj_dim),
                                   nn.BatchNorm1d(proj_dim),
                                   nn.ReLU(inplace=True),
                                   nn.Linear(proj_dim, proj_dim)
                                   )

    def forward(self, x):
        return self.model(x)

"""### **Code From augmentations.py**"""

from torchvision import transforms

train_transform = transforms.Compose([
    #transforms.ToPILImage(),
    transforms.RandomResizedCrop(32, scale=(0.2, 1.0)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),
    transforms.RandomGrayscale(p=0.2),
    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.5),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))
])
test_transform = transforms.Compose([
    #transforms.ToPILImage(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))
])

"""### **My Code**"""

import faiss
import torch
import torch.nn as nn
import numpy as np
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Subset
from tqdm import tqdm

from torchvision.datasets import CIFAR10
from torchvision.datasets import MNIST

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import roc_curve, auc, silhouette_score
from sklearn.cluster import KMeans

"""**Global Variables**"""

DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
SEED = 42
GAMMA = 1
EPSILON = 1e-4
LAMBDA = 25
MU = 25
NU = 1
PROJ_DIM = 512
ENCODER_DIM = 128
BATCH_SIZE = 256
LEARNING_RATE = 3e-4
BETAS = (0.9, 0.999)
WEIGHT_DECAY = 1e-6
NUM_OF_EPOCHS = 30
LP_NUM_OF_EPOCHS = 10
NUM_OF_CLASSES = 10

"""**Random Seed**"""

torch.manual_seed(SEED)          # Sets the seed for CPU
torch.cuda.manual_seed(SEED)     # Sets the seed for the current GPU

"""**Custom Classes**"""

class CombinedDataset(Dataset):
    def __init__(self, dataset1, dataset2):
        self.dataset1 = dataset1
        self.dataset2 = dataset2
        assert len(dataset1) == len(dataset2), "Datasets must have the same length"

    def __len__(self):
        return len(self.dataset1)

    def __getitem__(self, idx):
        sample1 = self.dataset1[idx]
        sample2 = self.dataset2[idx]
        return sample1, sample2


class VICReg(nn.Module):
    def __init__(self, encoder, projector):
        super(VICReg, self).__init__()
        self.encoder = encoder
        self.projector = projector

    def forward(self, images):
        encoder_reps = self.encoder(images)
        projections_reps = self.projector(encoder_reps)

        return projections_reps


class LinearProber(nn.Module):
    def __init__(self, encoder, D=ENCODER_DIM):
        super(LinearProber, self).__init__()
        self.encoder = encoder
        self.fc = nn.Linear(D, NUM_OF_CLASSES)

    def forward(self, x):
        x = self.encoder(x)
        x = self.fc(x)
        return x

"""**Auxiliary Functions**"""

def create_CIFAR10_dataloader(is_train=True, use_augmentations=True, batch_size=BATCH_SIZE):
    if use_augmentations:
        # Create datasets
        dataset1 = CIFAR10(root='./data', train=is_train, transform=train_transform, download=True)
        dataset2 = CIFAR10(root='./data', train=is_train, transform=train_transform, download=True)

        # Create dataloader
        dataloader = DataLoader(CombinedDataset(dataset1, dataset2), batch_size=batch_size, shuffle=is_train, num_workers=4)
    else:
        # Create dataset
        dataset = CIFAR10(root='./data', train=is_train, transform=test_transform, download=True)
        # Create dataloader
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=is_train, num_workers=4)

    return dataloader

def plot_VICReg_losses(train_objectives_losses, test_objectives_losses):
    # Plot match flow training losses as a function of epoch
    number_of_epoch = len(train_objectives_losses)
    epochs_list = list(range(1, number_of_epoch + 1))

    # Create plot
    losses_plot = make_subplots(rows=2, cols=2)
    for i in range(4):
        train_losses = [losses[i] for losses in train_objectives_losses]
        test_losses = [losses[i] for losses in test_objectives_losses]
        row = i//2 + 1
        col = i % 2 + 1

        losses_plot.add_trace(
            go.Scatter(x=epochs_list, y=train_losses, name=f"Train",
                       mode='lines+markers', line=dict(color='blue'), showlegend=(row == 0 and col == 0)), row=row, col=col
        )
        losses_plot.add_trace(
            go.Scatter(x=epochs_list, y=test_losses, name=f"Test",
                       mode='lines+markers', line=dict(color='red'), showlegend=(row == 0 and col == 0)), row=row, col=col
        )

    titles = {0: "Total Loss", 1: "Invariance Loss", 2: "Variance Loss", 3: "Covariance Loss"}
    for i in range(4):
        row = i//2 + 1
        col = i % 2 + 1
        losses_plot.update_xaxes(title_text="Number Of Epoch", row=row, col=col)
        losses_plot.update_yaxes(title_text=f"Average {titles[i]}", row=row, col=col)

    losses_plot.update_layout(
        height=800, width=1000,
        title_text="Train and Test average losses as functions of number of epoch",
        showlegend=True,
        legend=dict(x=1.02, y=1, traceorder='normal')
    )
    losses_plot.show()


def plot_2d_samples(samples, labels, use_CIFAR10_labels_bools, centers=None, title=""):
    # Validate parameters
    assert(labels.shape[0] == len(use_CIFAR10_labels_bools))
    assert(labels.shape[1] == samples.shape[0])
    assert(samples.shape[1] == 2)
    if centers is not None:
        assert(centers.shape[1] == 2)

    # Create a Figure
    num_of_plots = labels.shape[0]
    fig, axs = plt.subplots(nrows=1, ncols=num_of_plots, figsize=(14 * num_of_plots - 4, 7))

    # If nrows is 1 then axs will be 1D array instead of 2D array
    if num_of_plots == 1:
        axs = [axs]

    # Create a legends with distinct colors for each class
    unique_labels = np.unique(labels)
    labels_to_objects = {0: 'Airplane', 1: 'Automobile', 2: 'Bird', 3: 'Cat', 4: 'Deer', 5: 'Dog', 6: 'Frog', 7: 'Horse', 8: 'Ship', 9: 'Truck'}
    colormap = plt.get_cmap('tab10')
    norm = plt.Normalize(vmin=0, vmax=9)
    cifar10_handlers = []
    cluster_handlers = []

    for label in unique_labels:
        color = colormap(norm(label))
        cifar10_handlers.append(mpatches.Patch(color=color, label=labels_to_objects[label]))
        cluster_handlers.append(mpatches.Patch(color=color, label=f"Cluster #{label}"))

    # Create a scatter plots
    for i in range(num_of_plots):
        axs[i].scatter(samples[:, 0], samples[: , 1], c=labels[i, :], cmap='tab10', marker='o')
        # Mark centers in black
        if centers is not None:
            axs[i].scatter(centers[:, 0], centers[:, 1], c='black', marker='x', s=200)
        if use_CIFAR10_labels_bools[i]:
            axs[i].legend(handles=cifar10_handlers, title="Classes", bbox_to_anchor=(1.05, 1), loc='upper left')
            axs[i].set_title(f"Colored by class index")
        else:
            axs[i].legend(handles=cluster_handlers, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
            axs[i].set_title(f"Colored by cluster index")

    # Adjust layout
    plt.tight_layout()
    plt.subplots_adjust(top=0.9)
    plt.suptitle(title, y=1.05)

    plt.show()


def unnormalize_CIFAR10_image(image):
    """
    Unnormalize a tensor image taken from CIFAR10 database back to its
    original representation.
    """
    image = image.to(DEVICE)
    mean = torch.tensor([0.4914, 0.4822, 0.4465]).to(DEVICE)
    std = torch.tensor([0.2470, 0.2435, 0.2616]).to(DEVICE)
    image = image * std[:, None, None] + mean[:, None, None]

    return image


def present_neighbors_images(labels, selected_images, neighbors_tensor, title):
    # Validate parameters
    assert(len(labels) == selected_images.size(0))
    assert(len(labels) == neighbors_tensor.size(0))
    num_of_images = len(labels)
    num_of_neighbors = neighbors_tensor.size(1)

    # Create a legend with distinct colors for each class
    labels_to_objects = {0: 'Airplane', 1: 'Automobile', 2: 'Bird', 3: 'Cat', 4: 'Deer', 5: 'Dog', 6: 'Frog', 7: 'Horse', 8: 'Ship', 9: 'Truck'}

    # Create a figure
    fig, axs = plt.subplots(nrows=num_of_images, ncols=num_of_neighbors + 1, figsize=(13, 1.5 * num_of_images), gridspec_kw={'hspace': 0.5})
    fig.suptitle(title)

    # If nrows is 1 then axs will be 1D array instead of 2D array
    if num_of_images == 1:
        axs = [axs]

    # Create subplots
    for row in range(num_of_images):
        # Add row title
        axs[row][0].text(-1.5, 0.5, f"Label: {labels_to_objects[labels[row]]}", transform=axs[row][0].transAxes, fontsize=12, ha='right')

        # Add images
        axs[row][0].set_title("Selected Image", y=-0.5)
        image = unnormalize_CIFAR10_image(selected_images[row])
        axs[row][0].imshow(image.cpu().permute(1, 2, 0))
        axs[row][0].axis('off')

        for col in range(1, num_of_neighbors + 1):
            axs[row][col].set_title(f"Neighbor #{col}", y=-0.5)
            image = unnormalize_CIFAR10_image(neighbors_tensor[row, col - 1])
            axs[row][col].imshow(image.cpu().permute(1, 2, 0))
            axs[row][col].axis('off')

    plt.show()


def plot_ROC_curves(fpr_lst, tpr_lst, titles):
    # Validate parameters
    assert(len(titles) == len(fpr_lst))
    assert(len(fpr_lst) == len(tpr_lst))
    for i in range(len(fpr_lst)):
        assert(len(fpr_lst[i]) == len(tpr_lst[i]))

    # Create a figure
    fig = go.Figure()
    num_of_curves = len(fpr_lst)

    # Add the ROC curve fpr each row
    for index in range(num_of_curves):
        fig.add_trace(
            go.Scatter(x=fpr_lst[index], y=tpr_lst[index], name=titles[index], mode='lines+markers')
        )

    # Set plot title and labels
    fig.update_layout(title='ROC Curves',
                      xaxis_title='False Positive Rate',
                      yaxis_title='True Positive Rate',
                      autosize=False,
                      width=1300,
                      height=500,
                      )

    fig.show()


def plot_images(images_tensor, titles):
    # Validate parameters
    assert(len(titles) == images_tensor.size(0))

    # Initiate variables
    num_of_rows = images_tensor.size(0)
    num_of_images = images_tensor.size(1)

    # Create a figure
    fig = plt.figure(constrained_layout=True, figsize=(8, 0.5 * num_of_images))
    plt.subplots_adjust(top=1, bottom=1)

    # Create subfigures
    subfigs = fig.subfigures(nrows=num_of_rows, ncols=1)
    for row, subfig in enumerate(subfigs):
        subfig.suptitle(titles[row])
        subfig_axs = subfig.subplots(nrows=1, ncols=num_of_images, gridspec_kw={'hspace': 0.5})

        for col, ax in enumerate(subfig_axs):
            image = unnormalize_CIFAR10_image(images_tensor[row, col])
            ax.imshow(image.cpu().permute(1, 2, 0))
            ax.axis('off')

    plt.show()

"""## **Section 3 - VICReg**

**Quetion 1**
"""

def compute_VICReg_batch_losses(augm_images_batch1, augm_images_batch2, vicreg_model):
    # Compute projections for both batches
    proj_reps1 = vicreg_model(augm_images_batch1)
    proj_reps2 = vicreg_model(augm_images_batch2)

    # Compute the invariance objective loss
    inv_loss = torch.mean(torch.pow(proj_reps1 - proj_reps2, 2))

    # Compute the variance objective loss
    var_proj1 = torch.var(proj_reps1, dim=0)
    var_proj2 = torch.var(proj_reps2, dim=0)

    std_proj1 = torch.sqrt(var_proj1 + EPSILON)
    std_proj2 = torch.sqrt(var_proj2 + EPSILON)

    hinge_loss1 = torch.mean(torch.relu(GAMMA - std_proj1))
    hinge_loss2 = torch.mean(torch.relu(GAMMA - std_proj2))

    var_loss = hinge_loss1 + hinge_loss2

    # Compute the covariance objective loss
    cov_proj1 = torch.cov(proj_reps1.T)
    cov_proj2 = torch.cov(proj_reps2.T)

    assert(PROJ_DIM == cov_proj1.size(0))
    diag_mask = torch.eye(PROJ_DIM, device=DEVICE).bool()
    off_diagonal_cov_proj1 = cov_proj1.masked_select(~diag_mask)
    off_diagonal_cov_proj2 = cov_proj2.masked_select(~diag_mask)

    cov_proj1_loss = (1 / PROJ_DIM) * torch.sum(torch.pow(off_diagonal_cov_proj1, 2))
    cov_proj2_loss = (1 / PROJ_DIM) * torch.sum(torch.pow(off_diagonal_cov_proj2, 2))

    cov_loss = cov_proj1_loss + cov_proj2_loss

    return inv_loss, var_loss, cov_loss


def test_VICReg(test_loader, vicreg_model):
    # Change model mode to evaluation
    vicreg_model.eval()

    # Create losses variables
    inv_avg_loss = 0.
    var_avg_loss = 0.
    cov_avg_loss = 0.
    vicreg_avg_loss = 0.
    num_of_seen_batches = 0

    # Iterate the dataset
    for batch1, batch2 in tqdm(test_loader):
        # Get images batches and compute batch size
        augm_images_batch1 = batch1[0].to(DEVICE)
        augm_images_batch2 = batch2[0].to(DEVICE)

        # Compute losses on batch
        inv_loss, var_loss, cov_loss = compute_VICReg_batch_losses(augm_images_batch1, augm_images_batch2, vicreg_model)

        # Compute VICReg loss
        vicreg_loss = LAMBDA * inv_loss + MU * var_loss + NU * cov_loss

        # Update batch losses variables
        inv_avg_loss = (num_of_seen_batches * inv_avg_loss + inv_loss.item()) / (num_of_seen_batches + 1)
        var_avg_loss = (num_of_seen_batches * var_avg_loss + var_loss.item()) / (num_of_seen_batches + 1)
        cov_avg_loss = (num_of_seen_batches * cov_avg_loss + cov_loss.item()) / (num_of_seen_batches + 1)
        vicreg_avg_loss = (num_of_seen_batches * vicreg_avg_loss +  vicreg_loss.item()) / (num_of_seen_batches + 1)
        num_of_seen_batches += 1

    return vicreg_avg_loss, inv_avg_loss, var_avg_loss, cov_avg_loss


def train_VICReg(vicreg_model, train_loader, test_loader=None, num_epochs=NUM_OF_EPOCHS, lr=LEARNING_RATE, betas=BETAS, weight_decay=WEIGHT_DECAY, lam=LAMBDA, mu=MU, nu=NU, evaluate=False):
    # Validate parameters
    assert(not evaluate or test_loader!=None), "test dataloder must be given when using evaluation"

    # Move VICReg model to DEVICE
    vicreg_model = vicreg_model.to(DEVICE)

    # Initiate ADAM optimizer
    optimizer = optim.Adam(vicreg_model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)

    # Train + evaluation for each epoch
    train_objectives_losses = []
    test_objectives_losses = []

    for epoch in range(num_epochs):
        print(f"Train: Epoch {epoch + 1}")
        # Change model mode to train
        vicreg_model.train()

        # Create losses variables
        inv_avg_loss = 0.
        var_avg_loss = 0.
        cov_avg_loss = 0.
        vicreg_avg_loss = 0.
        num_of_seen_batches = 0

        # Iterate the dataset
        for batch1, batch2 in tqdm(train_loader):
            # Get images batches, move them to DEVICE and compute their size
            augm_images_batch1 = batch1[0].to(DEVICE)
            augm_images_batch2 = batch2[0].to(DEVICE)

            # Compute losses on batch
            inv_loss, var_loss, cov_loss = compute_VICReg_batch_losses(augm_images_batch1, augm_images_batch2, vicreg_model)

            # Compute VICReg loss
            vicreg_loss = lam * inv_loss + mu * var_loss + nu * cov_loss

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Backpropargate and update the model's parameters
            vicreg_loss.backward()
            optimizer.step()

            # Update batch losses variables
            if evaluate:
                with torch.no_grad():
                    inv_avg_loss = (num_of_seen_batches * inv_avg_loss + inv_loss.item()) / (num_of_seen_batches + 1)
                    var_avg_loss = (num_of_seen_batches * var_avg_loss + var_loss.item()) / (num_of_seen_batches + 1)
                    cov_avg_loss = (num_of_seen_batches * cov_avg_loss + cov_loss.item()) / (num_of_seen_batches + 1)
                    vicreg_avg_loss = (num_of_seen_batches * vicreg_avg_loss +  vicreg_loss.item()) / (num_of_seen_batches + 1)
                    num_of_seen_batches += 1

        # Update losses variables
        if evaluate:
            with torch.no_grad():
                # update train losses
                train_objectives_losses.append((vicreg_avg_loss, inv_avg_loss, var_avg_loss, cov_avg_loss))
                # update test losses
                test_objectives_losses.append(test_VICReg(test_loader, vicreg_model))

    return train_objectives_losses, test_objectives_losses

# Get train and test CIFAR10 data-loaders with augmentations
train_loader = create_CIFAR10_dataloader(is_train=True, use_augmentations=True)
test_loader = create_CIFAR10_dataloader(is_train=False, use_augmentations=True)

# Initiate encoder, projector and VICReg models
encoder_q1 = Encoder(D=ENCODER_DIM, device=DEVICE).to(DEVICE)
projector_q1 = Projector(D=ENCODER_DIM, proj_dim=PROJ_DIM).to(DEVICE)
vicreg_q1 = VICReg(encoder_q1, projector_q1).to(DEVICE)

# Train VICReg model and get train and test losses
train_objectives_losses, test_objectives_losses = train_VICReg(vicreg_model=vicreg_q1, train_loader=train_loader, test_loader=test_loader, evaluate=True)

# Plot losses
plot_VICReg_losses(train_objectives_losses, test_objectives_losses)

"""**Question 2**"""

def compute_images_representations(dataloader, encoder):
    # Set encoder mode to evaluation
    encoder.eval()

    # Initiate empty global lists
    data_img_encoders_lst = []
    data_labels_lst = []

    # Iterate the dataset and compute encoders
    for images, labels in tqdm(dataloader):
        images = images.to(DEVICE)
        labels = labels.to(DEVICE)

        # Compute encoders for images
        img_encoders = encoder(images)

        # Append results to global lists
        data_img_encoders_lst.append(img_encoders.detach())
        data_labels_lst.append(labels.detach())

    # Concatenate results
    data_img_encoders = torch.cat(data_img_encoders_lst, dim=0)
    data_labels = torch.cat(data_labels_lst, dim=0)

    return data_img_encoders, data_labels

def map_and_present_plot(data_encoder_reps, data_labels, method, use_CIFAR10_labels_bools, title, centers=None):
    # Validate method parameter
    assert(method in ['PCA', 'TSNE']), "Method must be either 'PCA' or 'TSNE'"
    assert(data_labels.dim() == 1 and len(use_CIFAR10_labels_bools) == 1 or len(use_CIFAR10_labels_bools) == data_labels.size(0))
    if centers is not None:
        assert(centers.size(1) == data_encoder_reps.size(1))

    # Unsqueeze data_labels if it has only one dimantion (required by plot_2d_samples)
    if data_labels.dim() == 1:
        data_labels = data_labels.unsqueeze(dim=0)

    # Prepare data representations to fit
    if centers is not None:
        data_to_fit = torch.cat([centers, data_encoder_reps], dim=0)
    else:
        data_to_fit = data_encoder_reps

    # Convert encoder representations and labels into numpy array
    data_labels_np = data_labels.to('cpu').numpy()
    data_to_fit_np = data_to_fit.to('cpu').numpy()

    if method == 'PCA':
        # Map encoder representations into 2D space using PCA methode
        pca_model = PCA(n_components=2, svd_solver='full')
        data_2d_reps = pca_model.fit_transform(data_to_fit_np)

    else:
        # Map encoder representations into 2D space using T-SNE methode
        tsne_model = TSNE(n_components=2, random_state=SEED)
        data_2d_reps = tsne_model.fit_transform(data_to_fit_np)

    if centers is not None:
        centers_2d = data_2d_reps[:centers.size(0), :]
        data_encoder_reps_2d = data_2d_reps[centers.size(0):, :]
    else:
        centers_2d = None
        data_encoder_reps_2d = data_2d_reps

    # Set a title
    title = f"{title} - 2D plot using {method} method"

    # Plot the new representations in 2D space
    plot_2d_samples(samples=data_encoder_reps_2d, labels=data_labels_np, use_CIFAR10_labels_bools=use_CIFAR10_labels_bools, centers=centers_2d ,title=title)


# Load CIFAR10 test dataset without augmentations
test_loader = create_CIFAR10_dataloader(is_train=False, use_augmentations=False)
# Compute new Encoder representation given to each image in the dataset
data_encoder_reps, data_labels = compute_images_representations(test_loader, encoder_q1)
# Map and plot representation to 2D space using PCA and T-SNE methods
title="VICReg's encoder representations (S3.Q1., with generated neighbors)"
map_and_present_plot(data_encoder_reps, data_labels, method='PCA', use_CIFAR10_labels_bools=[True], title=title)
map_and_present_plot(data_encoder_reps, data_labels, method='TSNE', use_CIFAR10_labels_bools=[True], title=title)

"""**Question 3**"""

def train_LinearProber(encoder, num_epochs=LP_NUM_OF_EPOCHS, lr=LEARNING_RATE, betas=BETAS, weight_decay=WEIGHT_DECAY):
    # Get CIFAR10 train data-loader without augmentations
    train_loader = create_CIFAR10_dataloader(is_train=True, use_augmentations=False)

    # Initiate Linear prober model
    lp_model = LinearProber(encoder=encoder, D=ENCODER_DIM).to(DEVICE)

    # Freeze encoder's parameters
    for param in lp_model.encoder.parameters():
        param.requires_grad = False

    # Set linear model mode to train and its internal encoder mode to evaluation
    lp_model.train()
    lp_model.encoder.eval()

    # Initiate ADAM optimizer and Cross Entropy criterion
    optimizer = optim.Adam(lp_model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)
    criterion = nn.CrossEntropyLoss()

    # Train the model
    for epoch in range(num_epochs):
        print(f"Train: Epoch {epoch + 1}")

        for images, labels in tqdm(train_loader):
            # Get images and labels batches and move them to DEVICE
            images = images.to(DEVICE)
            labels = labels.to(DEVICE)

            # Use the model to predict the images's labels
            pred_labels = lp_model(images)

            # Compute the loss using the criterion
            loss = criterion(pred_labels, labels)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Backpropargate and update the model's parameters
            loss.backward()
            optimizer.step()

    # Move model mode to evaluation
    lp_model.eval()
    return lp_model


def test_LinearProber(lp_model):
    # Get CIFAR10 test data-loader without augmentations
    test_loader = create_CIFAR10_dataloader(is_train=False, use_augmentations=False)

    # Set linear prober mode to evaluation
    lp_model.eval()

    # Calculate model accuracy
    num_of_correct_pred = 0
    num_of_seen_samples = 0

    for images, labels in tqdm(test_loader):
        # Get images and labels batches, move them to DEVICE and compute their size
        images = images.to(DEVICE)
        labels = labels.to(DEVICE)
        batch_size = images.size(0)

        # Use the model to predict the images's labels
        pred_labels = lp_model(images)

        # Calculate the number of correct prediction
        num_of_correct_pred += torch.sum(torch.argmax(pred_labels, dim=1) == labels).item()
        num_of_seen_samples += batch_size


    # Calculate the model accuracy
    accuracy = 100 * (num_of_correct_pred / num_of_seen_samples)

    return accuracy


# Train linear pober model
lp_model_q3 = train_LinearProber(encoder=encoder_q1)
# Test the linear prober model
lp_model_q3_accuracy = test_LinearProber(lp_model_q3)
# Report the linear prober accuracy
print(f"Linear prober accuracy using VICReg's encoder representations (S3.Q1., with generated neighbors): {lp_model_q3_accuracy:.4f}%")

"""**Question 4**"""

# Stage 1: Train new encoder with MU = 0 in the objective
# Get train CIFAR10 data-loaders with augmentations
train_loader = create_CIFAR10_dataloader(is_train=True, use_augmentations=True)

# Initiate encoder, projector and VICReg models
encoder_q4 = Encoder(D=ENCODER_DIM, device=DEVICE).to(DEVICE)
projector_q4 = Projector(D=ENCODER_DIM, proj_dim=PROJ_DIM).to(DEVICE)
vicreg_q4 = VICReg(encoder_q4, projector_q4).to(DEVICE)

# Train the VICReg model
train_VICReg(vicreg_model=vicreg_q4, train_loader=train_loader, mu=0., evaluate=False)

# Stage 2: Perform PCA visualization like in Q2
# Load CIFAR10 test dataset without augmentations
test_loader = create_CIFAR10_dataloader(is_train=False, use_augmentations=False)

# Compute new encoder representation given to each image in the dataset
data_img_encoders, data_labels = compute_images_representations(test_loader, encoder_q4)

# Map and plot representation into 2D space using PCA method
title="VICReg's encoder representations (S3.Q4., using generated neighbors and no variance objective)"
map_and_present_plot(data_img_encoders, data_labels, method='PCA', use_CIFAR10_labels_bools=['True'], title=title)


# Stage 3: Perform linear probing like in Q3
# Train linear pober model
lp_model_q4 = train_LinearProber(encoder=encoder_q4)
# Test the linear prober model
lp_model_q4_accuracy = test_LinearProber(lp_model_q4)
# Report the linear prober accuracy
print(f"Linear prober accuracy using VICReg's encoder representations (S3.Q4., with generated neighbors and no variance objective): {lp_model_q4_accuracy:.4f}%")

"""**Question 5**"""

def find_k_nearest_neighbors(samples_tensor, query_vectors, k, query_vectors_in_samples=False):
    # Validate parameters
    assert(k <= samples_tensor.size(0))
    assert(samples_tensor.size(1) == query_vectors.size(1))

    # Get the number of samples
    num_of_samples = samples_tensor.size(0)

    # Convert tensors to numpy arrays and ensure they are float32
    samples_np = samples_tensor.cpu().detach().numpy().astype(np.float32)
    query_vectors_np = query_vectors.cpu().detach().numpy().astype(np.float32)

    # Step 1: Build a FAISS L2 index
    index = faiss.IndexFlatL2(samples_np.shape[1])
    index.add(samples_np)

    # Step 2: Query the index to find the nearest neighbors
    distances, indices = index.search(query_vectors_np, k+1)

    # Exclude result of the query vector if they already included in the samples_tensor
    if query_vectors_in_samples:
        indices = indices[:, 1:]
        distances = distances[:, 1:]
    else:
        indices = indices[:, :k]
        distances = distances[:, :k]

    # Step3: Convert indices and distances numpy arrays into tensor and move to DEVICE
    indices = torch.from_numpy(indices).to(DEVICE)
    distances = torch.from_numpy(distances).to(DEVICE)

    return distances, indices


def create_CIFAR10_dataloader_using_knn(encoder, is_train):
    # Move encoder to DEVICE and set its mode to evaliation
    encoder = encoder.to(DEVICE)
    encoder.eval()

    # Create dataset
    orginal_dataset = CIFAR10(root='./data', train=is_train, transform=test_transform, download=True)
    orginal_dataloader = DataLoader(orginal_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    number_of_samples = len(orginal_dataset)

    # Compute the encoder representation of each image in the dataset
    data_img_encoder_reps, _ = compute_images_representations(orginal_dataloader, encoder)

    # Find the three nearest neighbors for each encoder representraion in the dataset
    _, nearest_neighbors_indices = find_k_nearest_neighbors(samples_tensor=data_img_encoder_reps, query_vectors=data_img_encoder_reps, k=3, query_vectors_in_samples=True) # size (number_of_samples, 3)

    # Randomly select one neighbor index for each sample in the dataset
    random_indices = torch.randint(0, 3, (number_of_samples, )).unsqueeze(dim=1).to(DEVICE)
    selected_neighbors_indices = torch.gather(nearest_neighbors_indices, 1, random_indices).squeeze(dim=1) #size (number_of_samples)

    # Create neighbors dataset according to the selected indices
    neighbors_dataset = Subset(dataset=orginal_dataset, indices=selected_neighbors_indices.cpu().tolist())

    # Combine orginal and neighbors datasets
    combined_dataset = CombinedDataset(orginal_dataset, neighbors_dataset)

    # Create a dataloader
    neighbors_dataloader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=is_train, num_workers=4)

    return neighbors_dataloader


# Step 1: Train a new encoder model using the nearest neighbor approach based on the encoder_q1's representations
# Get train neighbors dataloader
neighbors_dataloader = create_CIFAR10_dataloader_using_knn(encoder_q1, is_train=True)

# Initiate encoder, projector and VICReg models
encoder_q5 = Encoder(D=ENCODER_DIM, device=DEVICE).to(DEVICE)
projector_q5 = Projector(D=ENCODER_DIM, proj_dim=PROJ_DIM).to(DEVICE)
vicreg_q5 = VICReg(encoder_q5, projector_q5)

# Train the VICReg model
train_VICReg(vicreg_model=vicreg_q5, train_loader=neighbors_dataloader, num_epochs=1, evaluate=False)

# Stage 2: Perform linear probing like in Q3
# Train linear pober model
lp_model_q5 = train_LinearProber(encoder=encoder_q5)
# Test the linear prober model
lp_model_q5_accuracy = test_LinearProber(lp_model_q5)
# Report the linear prober accuracy
print(f"Linear prober accuracy using VICReg's encoder representations (S3.Q5., without generated neighbors): {lp_model_q5_accuracy:.4f}%")

"""**Question 7**"""

def find_k_distant_neighbors(samples_tensor, query_vectors, k):
    # Validate parameters
    assert(k <= samples_tensor.size(0))
    assert(samples_tensor.size(1) == query_vectors.size(1))

    # Get the number of samples and query vectors
    num_of_samples = samples_tensor.size(0)
    num_of_query_vectors = query_vectors.size(0)

    # Since FIASS L2 index dosn't support argmax we will convert the unputs according to the following formula
    # argmax ||x - y||^2 == argmax <(-2x, ||x||^2), (y, 1)>
    # FIASS Inner Product does support find argmax

    # Modify samples tensors
    L2_norm_square = torch.sum(torch.pow(samples_tensor, 2), dim=1).view(num_of_samples, 1)
    new_samples_tensors = torch.cat([-2 * samples_tensor, L2_norm_square], dim=1)

    # Modify query vectors
    ones_vector = torch.ones(num_of_query_vectors, 1).to(DEVICE)
    new_query_vectors = torch.cat([query_vectors, ones_vector], dim=1)

    # Convert tensors to numpy arrays and ensure they are float32
    new_samples_np = new_samples_tensors.cpu().detach().numpy().astype(np.float32)
    new_query_vectors_np = new_query_vectors.cpu().detach().numpy().astype(np.float32)

    # Step 1: Build a FAISS inner product index
    index = faiss.IndexFlatIP(new_samples_np.shape[1])
    index.add(new_samples_np)

    # Step 2: Query the index to find the nearest neighbors
    distances, indices = index.search(new_query_vectors_np, k)

    # Step3: Convert indices and distances numpy arrays into tensor and move to DEVICE
    indices = torch.from_numpy(indices).to(DEVICE)
    distances = torch.from_numpy(distances).to(DEVICE)

    # Step 4: Fix the distances tensor
    original_query_vectors_norms = torch.sum(torch.pow(query_vectors,2), dim=1)
    original_query_vectors_norms = torch.unsqueeze(original_query_vectors_norms, dim=1).repeat(1, k)
    distances = distances + original_query_vectors_norms

    return distances, indices


def select_classes_images(is_train=False):
    # Get CIFAR10 train dataset without augmentations
    dataset = CIFAR10(root='./data', train=is_train, transform=test_transform, download=True)

    # Shuffle samples selections
    shuffled_dataset_indices = np.arange(len(dataset))
    np.random.shuffle(shuffled_dataset_indices)

    # Initiate dictionary
    labels = list(range(NUM_OF_CLASSES))
    selected_images = {}

    for index in shuffled_dataset_indices:
        image, label = dataset[shuffled_dataset_indices[index]]
        if label not in selected_images.keys():
            selected_images[label] = image.unsqueeze(dim=0)
            if len(selected_images.keys()) == NUM_OF_CLASSES:
                break

    image_list = [selected_images[label].to(DEVICE) for label in range(NUM_OF_CLASSES)]
    selected_images_tensor = torch.cat(image_list, dim=0).to(DEVICE)

    return labels, selected_images_tensor


def find_images_neighbors_using_knn(dataset, encoder, selected_images_tensor, distant=False):
    # Set encoder mode to evaluation
    encoder = encoder.eval()

    # Initiate global lists
    number_of_selected_images = selected_images_tensor.size(0)
    all_neighbors_list = []

    # Compute the encoder representations of the images in the datasets
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    data_img_encoder_reps, _ = compute_images_representations(dataloader, encoder)

    # Compute the encoder representations of the selected images
    selected_img_encoder_reps = encoder(selected_images_tensor)

    # Find the 5 nearest/distant neighbors for each encoder representraion in the dataset
    if distant:
        _, neighbors_indices = find_k_distant_neighbors(samples_tensor=data_img_encoder_reps, query_vectors=selected_img_encoder_reps, k=5) # size (number_of_samples, 5)
    else:
        _, neighbors_indices = find_k_nearest_neighbors(samples_tensor=data_img_encoder_reps, query_vectors=selected_img_encoder_reps, k=5, query_vectors_in_samples=True) # size (number_of_samples, 5)

    # Build neighbors tensor for each selected images
    for i in range(number_of_selected_images):
        neighbors_images = [dataset[j][0].unsqueeze(dim=0) for j in neighbors_indices[i]]
        neighbors_images_tensor = torch.cat(neighbors_images, dim=0).unsqueeze(0)
        all_neighbors_list.append(neighbors_images_tensor)

    # Concatenate global lists into tensors
    all_neighbors_tensor = torch.cat(all_neighbors_list, dim=0)

    return all_neighbors_tensor


# Load train dataloader without augmentations
train_dataset = CIFAR10(root='./data', train=True, transform=test_transform, download=True)

# Select 10 random images from the training set, one from each class
labels, selected_images_tensor = select_classes_images(is_train=True)

# For each selected image, use encoder representations to find its five nearest neighbors in the dataset
nearest_neighbors_q1 = find_images_neighbors_using_knn(train_dataset, encoder_q1, selected_images_tensor, distant=False)
nearest_neighbors_q5 = find_images_neighbors_using_knn(train_dataset, encoder_q5, selected_images_tensor, distant=False)

# Plot the images together with their nearest neighbors
present_neighbors_images(labels, selected_images_tensor, nearest_neighbors_q1, title="Five nearest neighbors for VICReg's encoder representations (S3.Q1., with generated neighbors)")
present_neighbors_images(labels, selected_images_tensor, nearest_neighbors_q5, title="Five nearest neighbors for VICReg's encoder representations (S3.Q5., without generated neighbors)")

# For each selected image, use encoder representations to find its five distant neighbors in the dataset
distant_neighbors_q1 = find_images_neighbors_using_knn(train_dataset, encoder_q1, selected_images_tensor, distant=True)
distant_neighbors_q5 = find_images_neighbors_using_knn(train_dataset, encoder_q5, selected_images_tensor, distant=True)

# Plot the images together with their distant neighbors
present_neighbors_images(labels, selected_images_tensor, distant_neighbors_q1, title="Five distant neighbors using VICReg's encoder representations (S3.Q1., with generated neighbors):s")
present_neighbors_images(labels, selected_images_tensor, distant_neighbors_q5, title="Five distant neighbors for VICReg's encoder representations (S3.Q5., without generated neighbors)")

"""# **Section 4 - Downstream Applications**

## **4.1 - Anomaly Detection**

**Question 1**
"""

mnist_test_transform = transforms.Compose([
    # transforms.ToPILImage(), # Convert tensor to PIL Image
    transforms.Grayscale(num_output_channels=3), # Convert grayscale to RGB
    transforms.Resize((32, 32)), # Resize image to 32x32
    *test_transform.transforms # Use test_transform for CIFAR10 images
])

def compute_density_estimtion(cifar10_dataset, minst_dataset, encoder, k):
    # Create CIFAR10 train dataloaders
    cifar10_train_loader = create_CIFAR10_dataloader(is_train=True, use_augmentations=False)

    # Create test dataloader for CIFAR10 and MNIST datasets
    cifar10_test_loader = DataLoader(cifar10_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    minst_test_loader = DataLoader(minst_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    # Compute encoder presentations for each dataloader
    cifar10_train_encoder_reps, _ = compute_images_representations(cifar10_train_loader, encoder)
    cifar10_test_encoder_reps, _ = compute_images_representations(cifar10_test_loader, encoder)
    mnist_test_encoder_reps, _ = compute_images_representations(minst_test_loader, encoder)

    # Compute the (square) L2 distances of the 2 nearest neighbors of each test sample
    cifar10_test_distances, _ = find_k_nearest_neighbors(samples_tensor=cifar10_train_encoder_reps, query_vectors=cifar10_test_encoder_reps, k=k, query_vectors_in_samples=False)
    mnist_test_distances, _ = find_k_nearest_neighbors(samples_tensor=cifar10_train_encoder_reps, query_vectors=mnist_test_encoder_reps, k=k, query_vectors_in_samples=False)

    # Compute the density estimation for each test sample
    cifar10_test_density_estimation = (1 / k) * torch.sum(torch.sqrt(cifar10_test_distances), dim=1)
    mnist_test_density_estimation = (1 / k) * torch.sum(torch.sqrt(mnist_test_distances), dim=1)

    return cifar10_test_density_estimation, mnist_test_density_estimation


# Set the number of desired neighbors
number_of_neighbors = 2

# Creates datasets (initiate those datasets here to be used by Q3)
cifar10_test_dataset = CIFAR10(root='./data', train=False, transform=test_transform, download=True)
minst_test_dataset = MNIST(root='./data', train=False, transform=mnist_test_transform, download=True)

# Compute the density estimation for VICReg's encoder representations (S3.Q1., with generated neighbors)
cifar10_test_de_q1, mnist_test_de_q1 =  compute_density_estimtion(cifar10_dataset=cifar10_test_dataset, minst_dataset=minst_test_dataset,
                                                                  encoder=encoder_q1, k=number_of_neighbors)

# Compute the density estimation for VICReg's encoder representations (S3.Q5., without generated neighbors)
cifar10_test_de_q5, mnist_test_de_q5 =  compute_density_estimtion(cifar10_dataset=cifar10_test_dataset, minst_dataset=minst_test_dataset,
                                                                  encoder=encoder_q5, k=number_of_neighbors)

"""**Question 2**"""

# Create true labels array - sign images from CIFAR10 dataset as 0 and images from MNIST dataset as 1
true_labels = torch.cat([torch.zeros(cifar10_test_de_q1.size(0)), torch.ones(mnist_test_de_q1.size(0))], dim=0).cpu().numpy()

# Compute ROC curve for VICReg's encoder representations (S3.Q1., with generated neighbors)
density_estimations_q1 = torch.cat([cifar10_test_de_q1, mnist_test_de_q1], dim=0)
density_estimations_q1_np = density_estimations_q1.cpu().numpy()
fpr_q1, tpr_q1, _ = roc_curve(y_true=true_labels, y_score=density_estimations_q1_np, pos_label=1)

# Compute ROC curve for VICReg's encoder representations (S3.Q5., without generated neighbors)
density_estimations_q5 = torch.cat([cifar10_test_de_q5, mnist_test_de_q5], dim=0)
density_estimations_q5_np = density_estimations_q5.cpu().numpy()
fpr_q5, tpr_q5, _ = roc_curve(y_true=true_labels, y_score=density_estimations_q5_np, pos_label=1)

# Plot the ROC curves of each VICReg's encoder representation
fpr_lst = [fpr_q1, fpr_q5]
tpr_lst = [tpr_q1, tpr_q5]
titles=["ROC curve using VICReg's encoder representations (S3.Q1., with generated neighbors)", "ROC curve using VICReg's encoder representations (S3.Q5., with generated neighbors)"]
plot_ROC_curves(fpr_lst=fpr_lst, tpr_lst=tpr_lst, titles=titles)

# Compute the AUC of the ROC curve for each VICReg's encoder
auc_q1 = auc(fpr_q1, tpr_q1)
auc_q5 = auc(fpr_q5, tpr_q5)

print(f"AUC using VICReg's encoder representations (S3.Q1., with generated neighbors): {auc_q1:.4f}")
print(f"AUC using VICReg's encoder representations (S3.Q5., with generated neighbors): {auc_q5:.4f}")

"""**Question 3**"""

# Get the number of cifar10 images
number_of_cifar10_images = len(cifar10_test_dataset)
# Find the most anomalous images using VICReg's encoder representations (S3.Q1., with generated neighbors)
_, most_anomalous_q1_indices = torch.topk(density_estimations_q1, k=7, largest=True)
# Find the most anomalous images using VICReg's encoder representations (S3.Q5., without generated neighbors)
_, most_anomalous_q5_indices = torch.topk(density_estimations_q5, k=7, largest=True)

most_anomalous_q1_imgs = []
most_anomalous_q5_imgs = []

for i in range(7):
    index_q1 = most_anomalous_q1_indices[i]
    index_q5 = most_anomalous_q5_indices[i]

    if index_q1 > number_of_cifar10_images:
        most_anomalous_q1_imgs.append(minst_test_dataset[index_q1 - number_of_cifar10_images][0].unsqueeze(dim=0).to(DEVICE))
    else:
        most_anomalous_q1_imgs.append(cifar10_test_dataset[index_q1][0].unsqueeze(dim=0).to(DEVICE))

    if index_q5 > number_of_cifar10_images:
        most_anomalous_q5_imgs.append(minst_test_dataset[index_q5 - number_of_cifar10_images][0].unsqueeze(dim=0).to(DEVICE))
    else:
        most_anomalous_q5_imgs.append(cifar10_test_dataset[index_q5][0].unsqueeze(dim=0).to(DEVICE))

# Create a tensor of images
images_q1 = torch.cat(most_anomalous_q1_imgs, dim=0).unsqueeze(dim=0)
images_q5 = torch.cat(most_anomalous_q5_imgs, dim=0).unsqueeze(dim=0)
images_tensor = torch.cat([images_q1, images_q5], dim=0)

# Plot the images
titles=["7 most anomalous images using VICReg's encoder representations (S3.Q1., with generated neighbors)",
        "7 most anomalous images using VICReg's encoder representations (S3.Q5., without generated neighbors)"]
plot_images(images_tensor=images_tensor, titles=titles)

"""# **4.2 - Clustering**

**Question 1**
"""

# Create CIFAR10 train dataloaders
cifar10_train_loader = create_CIFAR10_dataloader(is_train=True, use_augmentations=False)

# Compute CIFAR10 training dataset representations using VICReg's encoder (S3.Q1., with generated neighbors)
data_img_encoder_q1_reps, data_img_encoder_q1_labels = compute_images_representations(cifar10_train_loader, encoder_q1)

# Compute CIFAR10 training dataset representations using VICReg's encoder (S3.Q5., without generated neighbors)
data_img_encoder_q5_reps, data_img_encoder_q5_labels = compute_images_representations(cifar10_train_loader, encoder_q5)

# Convert VICReg's encoders representations to numpy
data_img_encoder_q1_reps_np = data_img_encoder_q1_reps.cpu().numpy()
data_img_encoder_q5_reps_np = data_img_encoder_q5_reps.cpu().numpy()

# Create a KMeans object
kmeans = KMeans(n_clusters=10, random_state=0)

# Perform KMeans algorithm on the CIFAR10 training representations computed by VICReg's encoder representations (S3.Q1., with generated neighbors)
kmeans.fit(data_img_encoder_q1_reps_np)
kmeans_q1_centers = torch.from_numpy(kmeans.cluster_centers_).to(DEVICE)
kmeans_q1_labels = torch.from_numpy(kmeans.labels_).to(DEVICE)

# Perform KMeans algorithm on the CIFAR10 training representations computed by VICReg's encoder representations (S3.Q5., without generated neighbors)
kmeans.fit(data_img_encoder_q5_reps_np)
kmeans_q5_centers = torch.from_numpy(kmeans.cluster_centers_).to(DEVICE)
kmeans_q5_labels = torch.from_numpy(kmeans.labels_).to(DEVICE)

"""**Question 2**"""

# Create label tensors according to each encoder representation
encoder_q1_labels = torch.cat([kmeans_q1_labels.unsqueeze(dim=0), data_img_encoder_q1_labels.unsqueeze(dim=0)], dim=0)
encoder_q5_labels = torch.cat([kmeans_q5_labels.unsqueeze(dim=0), data_img_encoder_q5_labels.unsqueeze(dim=0)], dim=0)

# Map images VICReg's encoders representations into 2D space using TSNE method and plot two figures with diffrent labels
# (1) labeled according to clusters
# (2) labeles according to CIFAR10 classes

# VICReg's encoder representations (S3.Q1., with generated neighbors):
title="VICReg's encoder representations (S3.Q1., with generated neighbors)"
map_and_present_plot(data_encoder_reps=data_img_encoder_q1_reps, data_labels=encoder_q1_labels,
                     use_CIFAR10_labels_bools=[False, True], method='TSNE', centers=kmeans_q1_centers, title=title)

# VICReg's encoder representations (S3.Q5., without generated neighbors):
title="VICReg's encoder representations (S3.Q5., without generated neighbors)"
map_and_present_plot(data_encoder_reps=data_img_encoder_q5_reps, data_labels=encoder_q5_labels,
                     use_CIFAR10_labels_bools=[False, True], method='TSNE', centers=kmeans_q5_centers, title=title)

"""**Question 3**"""

# Compute the Silhouette score on the k-means clusters generated for each encoder
# Encoder from S3.Q1:
encoder_q1_score = silhouette_score(data_img_encoder_q1_reps_np, kmeans_q1_labels.cpu().numpy(), metric='l2')
print(f"Silhouette score for VICReg's encoder representations (S3.Q1., with generated neighbors): {encoder_q1_score:.4f}")
# Encoder from S3.Q5:
encoder_q5_score = silhouette_score(data_img_encoder_q5_reps_np, kmeans_q5_labels.cpu().numpy(), metric='l2')
print(f"Silhouette score for VICReg's encoder representations (S3.Q5., without generated neighbors): {encoder_q5_score:.4f}")
